ë‹¨ì¼ íì—ì„œ slurm accountingì„ ì´ìš©í•´ì„œ QoSë¥¼ ì ìš©í•˜ë©´ ìì›ì„ ì¢€ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì„ê²ƒ ê°™ì€ë°

ë‹¹ì‹  ë§ì´ ì™„ì „íˆ ë§ìŠµë‹ˆë‹¤! ë‹¨ì¼ íì—ì„œ Slurm Accountingê³¼ QoSë¥¼ ì˜ í™œìš©í•˜ë©´ ë” íš¨ìœ¨ì ì¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ğŸ”„ ì „ëµ ì¬í‰ê°€
ë‹¤ì¤‘ í vs ë‹¨ì¼ í + QoS
ì¸¡ë©´	ë‹¤ì¤‘ í	ë‹¨ì¼ í + QoS âœ…
ë¦¬ì†ŒìŠ¤ í™œìš©ë¥ 	70-80% (íë³„ ê³ ì • í• ë‹¹)	90-95% (ë™ì  ê³µìœ )
ìœ íœ´ ë¦¬ì†ŒìŠ¤	ë§ìŒ (í ê°„ ì´ë™ ë¶ˆê°€)	ì ìŒ (ì „ì²´ í’€ ê³µìœ )
ìš°ì„ ìˆœìœ„ ê´€ë¦¬	í ë ˆë²¨	ì„¸ë°€í•œ ì œì–´ (QoS)
ì„¤ì • ë³µì¡ë„	ë‚®ìŒ	ì¤‘ê°„
ìœ ì—°ì„±	ë‚®ìŒ	ë†’ìŒ
ODCR í™œìš©	ë¶€ë¶„ì 	ìµœëŒ€í™”

ê²°ë¡ : 100ê°œ ODCRì„ ìµœëŒ€í•œ í™œìš©í•˜ë ¤ë©´ ë‹¨ì¼ í + QoSê°€ ë” ë‚˜ìŒ!
ğŸ—ï¸ ë‹¨ì¼ í + QoS ì•„í‚¤í…ì²˜
ParallelCluster ì„¤ì • (ë‹¨ì¼ í)
Region: us-east-1
Image:
  Os: alinux2

HeadNode:
  InstanceType: c5.4xlarge
  Networking:
    SubnetId: subnet-12345678
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
  LocalStorage:
    RootVolume:
      Size: 100

Scheduling:
  Scheduler: slurm
  SlurmSettings:
    ScaledownIdletime: 10
    QueueUpdateStrategy: TERMINATE
    
    # Slurm Accounting í™œì„±í™”
    Database:
      Uri: mysql://slurm:password@mysql-endpoint:3306/slurm_acct_db
      UserName: slurm
      PasswordSecretArn: arn:aws:secretsmanager:us-east-1:123456789012:secret:slurm-db-pass
    
  SlurmQueues:
    # ============================================
    # ë‹¨ì¼ í†µí•© í - ëª¨ë“  ì›Œí¬ë¡œë“œ ê³µìœ 
    # ============================================
    - Name: compute
      CapacityType: ONDEMAND
      AllocationStrategy: lowest-price
      
      ComputeResources:
        # ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… í˜¼í•© (ìœ ì—°ì„±)
        - Name: a100-80gb
          InstanceType: p4de.24xlarge  # 8x A100 80GB
          MinCount: 0
          MaxCount: 50
          DisableSimultaneousMultithreading: true
          Efa:
            Enabled: true
            GdrSupport: true
        
        - Name: a100-40gb
          InstanceType: p4d.24xlarge   # 8x A100 40GB
          MinCount: 0
          MaxCount: 50
          DisableSimultaneousMultithreading: true
          Efa:
            Enabled: true
            GdrSupport: true
      
      Networking:
        SubnetIds:
          - subnet-12345678
        PlacementGroup:
          Enabled: true
      
      ComputeSettings:
        LocalStorage:
          RootVolume:
            Size: 300
          EphemeralVolume:
            MountDir: /scratch
      
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess

SharedStorage:
  - MountDir: /fsx
    Name: fsx-storage
    StorageType: FsxLustre
    FsxLustreSettings:
      StorageCapacity: 7200
      DeploymentType: PERSISTENT_2
      PerUnitStorageThroughput: 250
      DataCompressionType: LZ4

Monitoring:
  DetailedMonitoring: true
  Logs:
    CloudWatch:
      Enabled: true

ğŸ›ï¸ Slurm Accounting & QoS ì„¤ì •
1ï¸âƒ£ Slurm Accounting ì´ˆê¸°í™”
#!/bin/bash
# scripts/setup_slurm_accounting.sh
# í—¤ë“œ ë…¸ë“œì—ì„œ ì‹¤í–‰

# Slurm Accounting ë°ëª¬ ì‹œì‘
sudo systemctl enable slurmdbd
sudo systemctl start slurmdbd

# í´ëŸ¬ìŠ¤í„° ë“±ë¡
sacctmgr add cluster mycluster -i

# ê³„ì •(Account) ìƒì„±
sacctmgr add account ml_training Description="ML Training Projects" -i
sacctmgr add account ddp_projects Parent=ml_training -i
sacctmgr add account megatron_projects Parent=ml_training -i
sacctmgr add account deepspeed_projects Parent=ml_training -i

# ì‚¬ìš©ì ì¶”ê°€
sacctmgr add user myuser Account=ml_training -i

2ï¸âƒ£ QoS (Quality of Service) ì •ì˜
#!/bin/bash
# scripts/setup_qos.sh

# ============================================
# QoS 1: Critical (Megatron - ìµœê³  ìš°ì„ ìˆœìœ„)
# ============================================
sacctmgr add qos critical \
    Priority=10000 \
    MaxWall=7-00:00:00 \
    MaxJobsPerUser=2 \
    MaxSubmitJobsPerUser=5 \
    MaxNodesPerJob=50 \
    MaxCPUsPerUser=4800 \
    Flags=DenyOnLimit \
    Preempt=high,normal,low \
    -i

# ============================================
# QoS 2: High (DeepSpeed - ë†’ì€ ìš°ì„ ìˆœìœ„)
# ============================================
sacctmgr add qos high \
    Priority=5000 \
    MaxWall=3-00:00:00 \
    MaxJobsPerUser=3 \
    MaxSubmitJobsPerUser=8 \
    MaxNodesPerJob=40 \
    MaxCPUsPerUser=3840 \
    Flags=DenyOnLimit \
    Preempt=normal,low \
    -i

# ============================================
# QoS 3: Normal (DDP - ì¼ë°˜ ìš°ì„ ìˆœìœ„)
# ============================================
sacctmgr add qos normal \
    Priority=1000 \
    MaxWall=1-00:00:00 \
    MaxJobsPerUser=5 \
    MaxSubmitJobsPerUser=15 \
    MaxNodesPerJob=30 \
    MaxCPUsPerUser=2880 \
    Flags=DenyOnLimit \
    Preempt=low \
    -i

# ============================================
# QoS 4: Low (ì‹¤í—˜/ê°œë°œ - ë‚®ì€ ìš°ì„ ìˆœìœ„)
# ============================================
sacctmgr add qos low \
    Priority=100 \
    MaxWall=12:00:00 \
    MaxJobsPerUser=10 \
    MaxSubmitJobsPerUser=20 \
    MaxNodesPerJob=10 \
    MaxCPUsPerUser=960 \
    Flags=DenyOnLimit \
    -i

# ============================================
# QoS 5: Preemptible (ì„ ì  ê°€ëŠ¥ - ìµœì € ìš°ì„ ìˆœìœ„)
# ============================================
sacctmgr add qos preemptible \
    Priority=10 \
    MaxWall=6:00:00 \
    MaxJobsPerUser=20 \
    MaxNodesPerJob=5 \
    Flags=OverPartQOS \
    -i

echo "QoS ì„¤ì • ì™„ë£Œ!"
sacctmgr show qos format=Name,Priority,MaxWall,MaxNodes,MaxJobs,Preempt

3ï¸âƒ£ ê³„ì •ë³„ QoS í• ë‹¹
#!/bin/bash
# scripts/assign_qos.sh

# Megatron í”„ë¡œì íŠ¸ â†’ Critical QoS
sacctmgr modify account megatron_projects set QOS=critical -i
sacctmgr modify user myuser Account=megatron_projects set QOS=critical -i

# DeepSpeed í”„ë¡œì íŠ¸ â†’ High QoS
sacctmgr modify account deepspeed_projects set QOS=high -i
sacctmgr modify user myuser Account=deepspeed_projects set QOS=high -i

# DDP í”„ë¡œì íŠ¸ â†’ Normal QoS
sacctmgr modify account ddp_projects set QOS=normal -i
sacctmgr modify user myuser Account=ddp_projects set QOS=normal -i

# ê¸°ë³¸ QoS ì„¤ì •
sacctmgr modify user myuser set DefaultQOS=normal -i

echo "QoS í• ë‹¹ ì™„ë£Œ!"
sacctmgr show associations format=Account,User,QOS,DefaultQOS

ğŸš€ QoS ê¸°ë°˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
1ï¸âƒ£ Megatron (Critical QoS)
#!/bin/bash
# scripts/train_megatron_qos.sh

#SBATCH --job-name=megatron-gpt3
#SBATCH --partition=compute              # ë‹¨ì¼ í
#SBATCH --qos=critical                   # Critical QoS ì§€ì •
#SBATCH --account=megatron_projects      # Megatron ê³„ì •
#SBATCH --nodes=50
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --constraint=a100-80gb           # 80GB ë©”ëª¨ë¦¬ ìš”êµ¬
#SBATCH --exclusive
#SBATCH --time=7-00:00:00                # 7ì¼
#SBATCH --output=logs/megatron/%x_%j.out

echo "Job: $SLURM_JOB_ID"
echo "QoS: critical"
echo "Priority: 10000"
echo "Nodes: $SLURM_JOB_NUM_NODES"

export CUDA_DEVICE_MAX_CONNECTIONS=1
export NCCL_IB_DISABLE=0

declare -a CONTAINER_ARGS=(
    --container-image=/fsx/containers/megatron.sqsh
    --container-mounts=/fsx:/fsx
)

# 400 GPUs (50 nodes Ã— 8)
TENSOR_PARALLEL=8
PIPELINE_PARALLEL=8
# DATA_PARALLEL = 400 / (8 * 8) = 6.25 â‰ˆ 6

srun -l "${CONTAINER_ARGS[@]}" \
    pretrain_gpt.py \
    --tensor-model-parallel-size $TENSOR_PARALLEL \
    --pipeline-model-parallel-size $PIPELINE_PARALLEL \
    --num-layers 96 \
    --hidden-size 12288 \
    --num-attention-heads 96 \
    --micro-batch-size 1 \
    --global-batch-size 1536 \
    --train-iters 500000 \
    --data-path /fsx/data/megatron/gpt3 \
    --save /fsx/checkpoints/megatron \
    --fp16

2ï¸âƒ£ DeepSpeed (High QoS)
#!/bin/bash
# scripts/train_deepspeed_qos.sh

#SBATCH --job-name=deepspeed-bloom
#SBATCH --partition=compute              # ë‹¨ì¼ í
#SBATCH --qos=high                       # High QoS
#SBATCH --account=deepspeed_projects
#SBATCH --nodes=30
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --constraint=a100-40gb           # 40GBë¡œë„ ì¶©ë¶„
#SBATCH --exclusive
#SBATCH --time=3-00:00:00                # 3ì¼
#SBATCH --output=logs/deepspeed/%x_%j.out

echo "Job: $SLURM_JOB_ID"
echo "QoS: high"
echo "Priority: 5000"
echo "Nodes: $SLURM_JOB_NUM_NODES"

export NCCL_IB_DISABLE=0

declare -a CONTAINER_ARGS=(
    --container-image=/fsx/containers/deepspeed.sqsh
    --container-mounts=/fsx:/fsx
)

# 240 GPUs (30 nodes Ã— 8)
srun -l "${CONTAINER_ARGS[@]}" \
    --launcher=pytorch \
    train_deepspeed.py \
    --deepspeed \
    --deepspeed_config /fsx/configs/ds_config_zero3.json \
    --model bloom-176b \
    --data-path /fsx/data/pile \
    --output-dir /fsx/checkpoints/deepspeed \
    --per-device-train-batch-size 2 \
    --gradient-accumulation-steps 16 \
    --fp16

3ï¸âƒ£ DDP (Normal QoS)
#!/bin/bash
# scripts/train_ddp_qos.sh

#SBATCH --job-name=ddp-gpt2
#SBATCH --partition=compute              # ë‹¨ì¼ í
#SBATCH --qos=normal                     # Normal QoS
#SBATCH --account=ddp_projects
#SBATCH --nodes=10
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --constraint=a100-40gb
#SBATCH --exclusive
#SBATCH --time=1-00:00:00                # 1ì¼
#SBATCH --output=logs/ddp/%x_%j.out

echo "Job: $SLURM_JOB_ID"
echo "QoS: normal"
echo "Priority: 1000"
echo "Nodes: $SLURM_JOB_NUM_NODES"

export NCCL_DEBUG=INFO

declare -a CONTAINER_ARGS=(
    --container-image=/fsx/containers/ddp.sqsh
    --container-mounts=/fsx:/fsx
)

declare -a TORCHRUN_ARGS=(
    --nproc_per_node=8
    --nnodes=$SLURM_JOB_NUM_NODES
    --rdzv_id=$SLURM_JOB_ID
    --rdzv_backend=c10d
    --rdzv_endpoint=$(scontrol show hostname $SLURM_NODELIST | head -n1)
)

# 80 GPUs (10 nodes Ã— 8)
srun -l "${CONTAINER_ARGS[@]}" \
    torchrun "${TORCHRUN_ARGS[@]}" \
    train_ddp.py \
    --model gpt2-large \
    --data-path /fsx/data/wikitext \
    --output-dir /fsx/checkpoints/ddp \
    --batch-size 32

ğŸ¯ ê³ ê¸‰ QoS ê¸°ëŠ¥
1ï¸âƒ£ ì„ ì (Preemption) ì„¤ì •
# Critical QoSëŠ” ë‹¤ë¥¸ ëª¨ë“  ì‘ì—…ì„ ì„ ì  ê°€ëŠ¥
sacctmgr modify qos critical set Preempt=high,normal,low,preemptible -i

# ì„ ì  ì‹œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìŠ¤í¬ë¦½íŠ¸
# /opt/slurm/etc/slurm_prolog.sh
#!/bin/bash
if [ "$SLURM_JOB_PREEMPT" == "1" ]; then
    echo "Job being preempted - saving checkpoint..."
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¡œì§
fi

2ï¸âƒ£ Fair Share ì„¤ì •
# ê³„ì •ë³„ Fair Share ê°€ì¤‘ì¹˜
sacctmgr modify account megatron_projects set fairshare=50 -i
sacctmgr modify account deepspeed_projects set fairshare=30 -i
sacctmgr modify account ddp_projects set fairshare=20 -i

# Fair Share ìš°ì„ ìˆœìœ„ ê³„ì‚° í™œì„±í™”
# /opt/slurm/etc/slurm.conf
PriorityType=priority/multifactor
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=1000
PriorityWeightQOS=10000
PriorityMaxAge=7-0

3ï¸âƒ£ ë™ì  ë¦¬ì†ŒìŠ¤ ì œí•œ
# ì‹œê°„ëŒ€ë³„ QoS ì œí•œ ì¡°ì • (ì˜ˆ: ì—…ë¬´ ì‹œê°„ vs ì•¼ê°„)
# scripts/adjust_qos_limits.sh

#!/bin/bash
HOUR=$(date +%H)

if [ $HOUR -ge 9 ] && [ $HOUR -le 18 ]; then
    # ì—…ë¬´ ì‹œê°„: ì œí•œ ì™„í™”
    sacctmgr modify qos normal set MaxNodesPerJob=40 -i
    sacctmgr modify qos low set MaxNodesPerJob=20 -i
else
    # ì•¼ê°„: ì œí•œ ê°•í™” (ë” ë§ì€ ë¦¬ì†ŒìŠ¤ í—ˆìš©)
    sacctmgr modify qos normal set MaxNodesPerJob=50 -i
    sacctmgr modify qos low set MaxNodesPerJob=30 -i
fi

# Cronìœ¼ë¡œ ìë™ ì‹¤í–‰
# 0 9,18 * * * /path/to/adjust_qos_limits.sh

4ï¸âƒ£ ë²„ìŠ¤íŒ…(Bursting) í—ˆìš©
# íŠ¹ì • QoSì— ë²„ìŠ¤íŒ… í—ˆìš© (ì¼ì‹œì ìœ¼ë¡œ ì œí•œ ì´ˆê³¼)
sacctmgr modify qos high set \
    GrpTRESMins=cpu=1000000 \
    MaxTRESPerUser=cpu=5000 \
    -i

# ì‚¬ìš©ìê°€ ì¼ì • í¬ë ˆë”§ ë‚´ì—ì„œ ì œí•œ ì´ˆê³¼ ê°€ëŠ¥

ğŸ“Š ë¦¬ì†ŒìŠ¤ í™œìš© ìµœì í™” ì‹œë‚˜ë¦¬ì˜¤
ì‹œë‚˜ë¦¬ì˜¤ 1: ëª¨ë“  ì‘ì—… ë™ì‹œ ì‹¤í–‰ (ì´ìƒì )
ì‹œê°„: 00:00
ìƒíƒœ: 
â”œâ”€ Megatron (Critical): 50 ë…¸ë“œ ì‚¬ìš© ì¤‘
â”œâ”€ DeepSpeed (High):    30 ë…¸ë“œ ì‚¬ìš© ì¤‘
â””â”€ DDP (Normal):        20 ë…¸ë“œ ì‚¬ìš© ì¤‘

ì´ ì‚¬ìš©: 100 ë…¸ë“œ (100% í™œìš©) âœ…

ì‹œë‚˜ë¦¬ì˜¤ 2: Critical ì‘ì—… ì¶”ê°€ ë„ì°©
ì‹œê°„: 02:00
ì´ë²¤íŠ¸: ìƒˆë¡œìš´ Critical ì‘ì—… ì œì¶œ (20 ë…¸ë“œ í•„ìš”)

Slurm ë™ì‘:
1. Normal QoS ì‘ì—… 10ê°œ ì„ ì  (ì²´í¬í¬ì¸íŠ¸ ì €ì¥)
2. Low QoS ì‘ì—… 10ê°œ ì„ ì 
3. 20 ë…¸ë“œ í™•ë³´
4. Critical ì‘ì—… ì¦‰ì‹œ ì‹œì‘

ê²°ê³¼:
â”œâ”€ Megatron (Critical): 70 ë…¸ë“œ (50 + 20)
â”œâ”€ DeepSpeed (High):    30 ë…¸ë“œ (ìœ ì§€)
â””â”€ DDP (Normal):        0 ë…¸ë“œ (ì„ ì ë¨, ëŒ€ê¸° ì¤‘)

ì´ ì‚¬ìš©: 100 ë…¸ë“œ (100% í™œìš©) âœ…

ì‹œë‚˜ë¦¬ì˜¤ 3: Critical ì‘ì—… ì™„ë£Œ í›„
ì‹œê°„: 06:00
ì´ë²¤íŠ¸: Critical ì‘ì—… 1ê°œ ì™„ë£Œ (50 ë…¸ë“œ í•´ì œ)

Slurm ë™ì‘:
1. ëŒ€ê¸° ì¤‘ì¸ Normal ì‘ì—… ì¬ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ)
2. ë‚¨ì€ ë¦¬ì†ŒìŠ¤ë¡œ Low ì‘ì—… ì‹œì‘

ê²°ê³¼:
â”œâ”€ Megatron (Critical): 20 ë…¸ë“œ
â”œâ”€ DeepSpeed (High):    30 ë…¸ë“œ
â”œâ”€ DDP (Normal):        30 ë…¸ë“œ (ì¬ì‹œì‘)
â””â”€ Experimental (Low):  20 ë…¸ë“œ (ìƒˆë¡œ ì‹œì‘)

ì´ ì‚¬ìš©: 100 ë…¸ë“œ (100% í™œìš©) âœ…

ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„
1ï¸âƒ£ ì‹¤ì‹œê°„ QoS ëª¨ë‹ˆí„°ë§
#!/bin/bash
# scripts/monitor_qos.sh

watch -n 5 '
echo "=== Slurm QoS Status ==="
echo ""

echo "=== Queue Status ==="
squeue -o "%.10i %.12j %.10u %.10a %.10q %.8T %.10M %.6D %R" --sort=-p,q

echo ""
echo "=== QoS Usage Summary ==="
for qos in critical high normal low preemptible; do
    running=$(squeue -q $qos -t RUNNING -h | wc -l)
    pending=$(squeue -q $qos -t PENDING -h | wc -l)
    nodes=$(squeue -q $qos -t RUNNING -h -o "%D" | awk "{sum+=\$1} END {print sum}")
    echo "$qos: Running=$running, Pending=$pending, Nodes=$nodes"
done

echo ""
echo "=== Fair Share Status ==="
sshare -A

echo ""
echo "=== Resource Utilization ==="
sinfo -o "%20P %5a %10l %6D %6t %10C"

echo ""
echo "=== Top Priority Jobs ==="
sprio -o "%.10i %.10u %.10q %.10Y %.10F %.10J %.10P %.10Q" | head -n 10
'

2ï¸âƒ£ ë¦¬ì†ŒìŠ¤ í™œìš©ë¥  ë¶„ì„
#!/bin/bash
# scripts/analyze_utilization.sh

# ì§€ë‚œ 24ì‹œê°„ í†µê³„
sacct -S $(date -d '1 day ago' +%Y-%m-%d) \
      -o JobID,JobName,Account,QOS,AllocNodes,Elapsed,State \
      --format=JobID,JobName,Account,QOS,AllocNodes,Elapsed,State

# QoSë³„ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© í†µê³„
echo ""
echo "=== QoS Resource Usage (Last 24h) ==="
for qos in critical high normal low; do
    echo "--- $qos ---"
    sacct -S $(date -d '1 day ago' +%Y-%m-%d) \
          -X -q $qos \
          --format=AllocNodes,CPUTime,Elapsed \
          | awk 'NR>2 {nodes+=$1; cputime+=$2} END {print "Total Nodes:", nodes, "Total CPU Hours:", cputime/3600}'
done

# í‰ê·  ëŒ€ê¸° ì‹œê°„
echo ""
echo "=== Average Wait Time by QoS ==="
for qos in critical high normal low; do
    avg_wait=$(sacct -S $(date -d '1 day ago' +%Y-%m-%d) \
                     -X -q $qos -s COMPLETED \
                     --format=Submit,Start \
                     | awk 'NR>2 {/* ëŒ€ê¸° ì‹œê°„ ê³„ì‚° */}')
    echo "$qos: $avg_wait"
done

3ï¸âƒ£ CloudWatch í†µí•©
#!/usr/bin/env python3
# scripts/cloudwatch_metrics.py

import boto3
import subprocess
import time

cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')

def get_slurm_metrics():
    """Slurm ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    metrics = {}
    
    # QoSë³„ ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìˆ˜
    for qos in ['critical', 'high', 'normal', 'low']:
        result = subprocess.run(
            ['squeue', '-q', qos, '-t', 'RUNNING', '-h'],
            capture_output=True, text=True
        )
        metrics[f'{qos}_running'] = len(result.stdout.strip().split('\n'))
    
    # ì „ì²´ ë…¸ë“œ ì‚¬ìš©ë¥ 
    result = subprocess.run(
        ['sinfo', '-h', '-o', '%D %t'],
        capture_output=True, text=True
    )
    total_nodes = 0
    allocated_nodes = 0
    for line in result.stdout.strip().split('\n'):
        count, state = line.split()
        total_nodes += int(count)
        if state == 'alloc':
            allocated_nodes += int(count)
    
    metrics['node_utilization'] = (allocated_nodes / total_nodes * 100) if total_nodes > 0 else 0
    
    return metrics

def send_to_cloudwatch(metrics):
    """CloudWatchì— ë©”íŠ¸ë¦­ ì „ì†¡"""
    metric_data = []
    
    for metric_name, value in metrics.items():
        metric_data.append({
            'MetricName': metric_name,
            'Value': value,
            'Unit': 'Count' if 'running' in metric_name else 'Percent',
            'Timestamp': time.time()
        })
    
    cloudwatch.put_metric_data(
        Namespace='SlurmCluster',
        MetricData=metric_data
    )

if __name__ == '__main__':
    while True:
        metrics = get_slurm_metrics()
        send_to_cloudwatch(metrics)
        print(f"Sent metrics: {metrics}")
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤

ğŸ’° ë¹„ìš© ìµœì í™” (ë‹¨ì¼ í + QoS)
ODCR 100ê°œ ì¸ìŠ¤í„´ìŠ¤ ìµœì  í™œìš©
# ì‹¤ì‹œê°„ ë¦¬ì†ŒìŠ¤ ì¬ë¶„ë°° ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# scripts/optimize_resources.sh

# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë…¸ë“œ ìˆ˜ í™•ì¸
CRITICAL_NODES=$(squeue -q critical -t RUNNING -h -o "%D" | awk '{sum+=$1} END {print sum}')
HIGH_NODES=$(squeue -q high -t RUNNING -h -o "%D" | awk '{sum+=$1} END {print sum}')
NORMAL_NODES=$(s

Advanced
Connected
