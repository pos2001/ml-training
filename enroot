â€œDocker ì´ë¯¸ì§€ ë¹Œë“œ â†’ ECR ë“±ë¡ â†’ í—¤ë“œë…¸ë“œì—ì„œ pull â†’ sqsh ë³€í™˜â€
ParallelCluster ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë¶„ì‚° íŠ¸ë ˆì´ë‹ ì™„ì „ ê°€ì´ë“œ

ë„¤, ì œì‹œí•˜ì‹  ì›Œí¬í”Œë¡œìš°ê°€ ë§ìŠµë‹ˆë‹¤! ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

1ë‹¨ê³„: ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì¤€ë¹„

1.1 Dockerfile ì‘ì„±

Dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04

ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-dev \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

PyTorch ë° ë¶„ì‚° íŠ¸ë ˆì´ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
RUN pip3 install --no-cache-dir \
    torch torchvision torchaudio \
    transformers \
    accelerate \
    deepspeed \
    pytorch-lightning \
    wandb

NCCL ë° MPI ì„¤ì •
ENV NCCL_DEBUG=INFO
ENV NCCL_SOCKET_IFNAME=^docker0,lo

ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /workspace

íŠ¸ë ˆì´ë‹ ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬
COPY train.py /workspace/
COPY requirements.txt /workspace/

ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜
RUN pip3 install -r requirements.txt

ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ì„¤ì •
ENTRYPOINT ["python3"]
CMD ["train.py"]


1.2 ë¡œì»¬ì—ì„œ ì´ë¯¸ì§€ ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸

ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t my-training-container:latest .

ë¡œì»¬ í…ŒìŠ¤íŠ¸
docker run --gpus all \
  -v $(pwd)/data:/workspace/data \
  -v $(pwd)/output:/workspace/output \
  my-training-container:latest train.py --test


1.3 ECRì— í‘¸ì‹œ

ECR ë¡œê·¸ì¸
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com

ECR ë¦¬í¬ì§€í† ë¦¬ ìƒì„± (ìµœì´ˆ 1íšŒ)
aws ecr create-repository --repository-name ml-training

ì´ë¯¸ì§€ íƒœê¹…
docker tag my-training-container:latest \
  ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/ml-training:latest

ECRì— í‘¸ì‹œ
docker push ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/ml-training:latest


2ë‹¨ê³„: ParallelCluster í—¤ë“œë…¸ë“œì—ì„œ ì»¨í…Œì´ë„ˆ ì¤€ë¹„

2.1 ECRì—ì„œ ì´ë¯¸ì§€ Pull ë° ë³€í™˜

#!/bin/bash
prepare_container.sh

set -e

ECR_URI="${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com"
CONTAINER_NAME="ml-training"
TAG="latest"
FSX_CONTAINER_DIR="/fsx/containers"

echo "=== 1. ECR ë¡œê·¸ì¸ ==="
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin ${ECR_URI}

echo "=== 2. ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ Pull ==="
docker pull ${ECR_URI}/${CONTAINER_NAME}:${TAG}

echo "=== 3. Enrootë¡œ ë³€í™˜ ==="
ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (ì¶©ë¶„í•œ ê³µê°„ í™•ë³´)
mkdir -p /tmp/enroot
export ENROOT_TEMP_PATH=/tmp/enroot

sqsh íŒŒì¼ë¡œ ë³€í™˜
enroot import -o /tmp/${CONTAINER_NAME}.sqsh \
  docker://${ECR_URI}/${CONTAINER_NAME}:${TAG}

echo "=== 4. FSxë¡œ ì´ë™ ==="
FSx ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
sudo mkdir -p ${FSX_CONTAINER_DIR}
sudo chmod 755 ${FSX_CONTAINER_DIR}

ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì´ë™
sudo mv /tmp/${CONTAINER_NAME}.sqsh ${FSX_CONTAINER_DIR}/

ê¶Œí•œ ì„¤ì •
sudo chmod 644 ${FSX_CONTAINER_DIR}/${CONTAINER_NAME}.sqsh

echo "=== 5. ì •ë¦¬ ==="
rm -rf /tmp/enroot
docker rmi ${ECR_URI}/${CONTAINER_NAME}:${TAG}

echo "ì™„ë£Œ! ì»¨í…Œì´ë„ˆ ìœ„ì¹˜: ${FSX_CONTAINER_DIR}/${CONTAINER_NAME}.sqsh"


2.2 ì‹¤í–‰

ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x prepare_container.sh

í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ ì‹¤í–‰
export ACCOUNT_ID="123456789012"
./prepare_container.sh


3ë‹¨ê³„: ë¶„ì‚° íŠ¸ë ˆì´ë‹ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

3.1 Slurm ì‘ì—… ìŠ¤í¬ë¦½íŠ¸

#!/bin/bash
#SBATCH --job-name=distributed-training
#SBATCH --nodes=4                    # 4ê°œ ë…¸ë“œ ì‚¬ìš©
#SBATCH --ntasks-per-node=8          # ë…¸ë“œë‹¹ 8ê°œ GPU (p4d.24xlarge ê¸°ì¤€)
#SBATCH --cpus-per-task=12           # GPUë‹¹ CPU ì½”ì–´
#SBATCH --gres=gpu:8                 # ë…¸ë“œë‹¹ GPU ìˆ˜
#SBATCH --time=24:00:00              # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„
#SBATCH --output=/fsx/logs/%j.out
#SBATCH --error=/fsx/logs/%j.err

í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CONTAINER_IMAGE="/fsx/containers/ml-training.sqsh"
export DATA_DIR="/fsx/data"
export OUTPUT_DIR="/fsx/output/${SLURM_JOB_ID}"
export CHECKPOINT_DIR="/fsx/checkpoints/${SLURM_JOB_ID}"

ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ${OUTPUT_DIR}
mkdir -p ${CHECKPOINT_DIR}

NCCL ì„¤ì • (EFA í™œìš©)
export NCCL_DEBUG=INFO
export NCCL_PROTO=simple
export FI_PROVIDER=efa
export FI_EFA_USE_DEVICE_RDMA=1
export NCCL_SOCKET_IFNAME=^docker0,lo

ë§ˆìŠ¤í„° ë…¸ë“œ ì •ë³´
export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)
export MASTER_PORT=29500

ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜
export WORLD_SIZE=$((SLURM_NNODES * SLURM_NTASKS_PER_NODE))

echo "=== Job Information ==="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Nodes: ${SLURM_NNODES}"
echo "GPUs per node: ${SLURM_NTASKS_PER_NODE}"
echo "Total GPUs: ${WORLD_SIZE}"
echo "Master: ${MASTER_ADDR}:${MASTER_PORT}"
echo "======================="

Enroot/Pyxisë¥¼ í†µí•œ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
srun --container-image=${CONTAINER_IMAGE} \
     --container-mounts=${DATA_DIR}:/data,${OUTPUT_DIR}:/output,${CHECKPOINT_DIR}:/checkpoints \
     --container-workdir=/workspace \
     python3 train.py \
       --data_dir /data \
       --output_dir /output \
       --checkpoint_dir /checkpoints \
       --batch_size 32 \
       --learning_rate 1e-4 \
       --num_epochs 100 \
       --distributed


3.2 PyTorch ë¶„ì‚° íŠ¸ë ˆì´ë‹ ì½”ë“œ ì˜ˆì‹œ

train.py
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup_distributed():
    """ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”"""
    # Slurm í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    rank = int(os.environ.get('SLURM_PROCID', 0))
    world_size = int(os.environ.get('SLURM_NTASKS', 1))
    local_rank = int(os.environ.get('SLURM_LOCALID', 0))
    
    # ë§ˆìŠ¤í„° ì£¼ì†Œ ì„¤ì •
    master_addr = os.environ.get('MASTER_ADDR', 'localhost')
    master_port = os.environ.get('MASTER_PORT', '29500')
    
    # ë¶„ì‚° ì´ˆê¸°í™”
    dist.init_process_group(
        backend='nccl',
        init_method=f'tcp://{master_addr}:{master_port}',
        world_size=world_size,
        rank=rank
    )
    
    # GPU ì„¤ì •
    torch.cuda.set_device(local_rank)
    
    return rank, world_size, local_rank

def cleanup_distributed():
    """ë¶„ì‚° í™˜ê²½ ì •ë¦¬"""
    dist.destroy_process_group()

def main():
    # ë¶„ì‚° ì„¤ì •
    rank, world_size, local_rank = setup_distributed()
    
    if rank == 0:
        print(f"Starting distributed training on {world_size} GPUs")
    
    # ëª¨ë¸ ìƒì„± ë° GPUë¡œ ì´ë™
    model = YourModel().cuda(local_rank)
    model = DDP(model, device_ids=[local_rank])
    
    # ë°ì´í„°ë¡œë” (DistributedSampler ì‚¬ìš©)
    train_dataset = YourDataset()
    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank
    )
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )
    
    # íŠ¸ë ˆì´ë‹ ë£¨í”„
    for epoch in range(args.num_epochs):
        train_sampler.set_epoch(epoch)  # ì¤‘ìš”!
        
        for batch in train_loader:
            # íŠ¸ë ˆì´ë‹ ì½”ë“œ
            pass
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (rank 0ë§Œ)
        if rank == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, f'/checkpoints/checkpoint_epoch_{epoch}.pt')
    
    cleanup_distributed()

if __name__ == '__main__':
    main()


4ë‹¨ê³„: ì‘ì—… ì œì¶œ ë° ëª¨ë‹ˆí„°ë§

ì‘ì—… ì œì¶œ
sbatch distributed_training.sh

ì‘ì—… ìƒíƒœ í™•ì¸
squeue -u $USER

ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f /fsx/logs/<job_id>.out

GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ (íŠ¹ì • ë…¸ë“œì—ì„œ)
srun --jobid=<job_id> --nodes=1 --ntasks=1 nvidia-smi


FSx ë””ë ‰í† ë¦¬ êµ¬ì¡° ì˜ˆì‹œ

/fsx/
â”œâ”€â”€ containers/
â”‚   â”œâ”€â”€ ml-training.sqsh          # ë©”ì¸ íŠ¸ë ˆì´ë‹ ì»¨í…Œì´ë„ˆ
â”‚   â”œâ”€â”€ preprocessing.sqsh         # ì „ì²˜ë¦¬ìš© ì»¨í…Œì´ë„ˆ
â”‚   â””â”€â”€ inference.sqsh             # ì¶”ë¡ ìš© ì»¨í…Œì´ë„ˆ
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ validation/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ 12345/                     # Job IDë³„ ì¶œë ¥
â”‚   â””â”€â”€ 12346/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ 12345/
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_0.pt
â”‚   â”‚   â””â”€â”€ checkpoint_epoch_1.pt
â”‚   â””â”€â”€ 12346/
â””â”€â”€ logs/
    â”œâ”€â”€ 12345.out
    â”œâ”€â”€ 12345.err
    â”œâ”€â”€ 12346.out
    â””â”€â”€ 12346.err


ì£¼ìš” ì²´í¬í¬ì¸íŠ¸

ì»¨í…Œì´ë„ˆ ì¤€ë¹„ ì‹œ:
- âœ… ì¶©ë¶„í•œ ì„ì‹œ ì €ì¥ ê³µê°„ (í—¤ë“œë…¸ë“œì— 50GB+ ê¶Œì¥)
- âœ… ECR ì ‘ê·¼ ê¶Œí•œ í™•ì¸
- âœ… Enroot ì„¤ì¹˜ í™•ì¸
- âœ… FSx ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸

ë¶„ì‚° íŠ¸ë ˆì´ë‹ ì‹œ:
- âœ… NCCL ë° EFA ì„¤ì •
- âœ… DistributedSampler ì‚¬ìš©
- âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (rank 0ë§Œ)
- âœ… ì ì ˆí•œ ë°°ì¹˜ í¬ê¸° ì„¤ì •

ì„±ëŠ¥ ìµœì í™”:
- âœ… Mixed precision training (AMP)
- âœ… Gradient accumulation
- âœ… ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
- âœ… FSx Lustre ìŠ¤íŠ¸ë¼ì´í•‘ ì„¤ì •

ì´ ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•´ ParallelClusterì—ì„œ ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ëŒ€ê·œëª¨ ë¶„ì‚° íŠ¸ë ˆì´ë‹ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

> ğŸ“ ì§€ì‹ ì¶œì²˜: ì´ ê°€ì´ë“œëŠ” AWS ParallelCluster, Slurm, PyTorch ë¶„ì‚° íŠ¸ë ˆì´ë‹ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì‹  ParallelCluster ì„¤ì • ë° AWS ì„œë¹„ìŠ¤ë³„ ì„¸ë¶€ ì‚¬í•­ì€ RAG ëª¨ë“œë¥¼ í™œì„±í™”í•˜ì—¬ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
