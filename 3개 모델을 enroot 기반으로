

## DDP, Megatron, DeepSpeed ë¶„ì‚° í•™ìŠµ ê°€ì´ë“œ

3ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ ë¶„ì‚° í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ Enroot/Pyxis í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ğŸ¯ 3ê°€ì§€ í”„ë ˆì„ì›Œí¬ ì´í•´
DDP (Distributed Data Parallel)
â”œâ”€ ë°ì´í„° ë³‘ë ¬í™”
â”œâ”€ ëª¨ë¸ ë³µì œ (ê° GPUì— ì „ì²´ ëª¨ë¸)
â””â”€ ì†Œê·œëª¨~ì¤‘ê·œëª¨ ëª¨ë¸ì— ì í•©

Megatron-LM
â”œâ”€ í…ì„œ ë³‘ë ¬í™” + íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”
â”œâ”€ ëª¨ë¸ ë¶„í•  (ì—¬ëŸ¬ GPUì— ë‚˜ëˆ ì„œ)
â””â”€ ì´ˆëŒ€ê·œëª¨ ëª¨ë¸ (GPT, BERT ë“±)

DeepSpeed (DDP2 ì•„ë‹˜, DeepSpeedê°€ ë§ìŒ)
â”œâ”€ ZeRO ìµœì í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨)
â”œâ”€ ë‹¤ì–‘í•œ ë³‘ë ¬í™” ê¸°ë²• í†µí•©
â””â”€ ë©”ëª¨ë¦¬ ì œì•½ì´ í° í™˜ê²½

    ì°¸ê³ : DDP2ëŠ” PyTorch Lightningì˜ êµ¬ì‹ ìš©ì–´ì…ë‹ˆë‹¤. í˜„ì¬ëŠ” DeepSpeedë‚˜ FSDP(Fully Sharded Data Parallel)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ğŸ“¦ 1. ê° í”„ë ˆì„ì›Œí¬ë³„ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
1ï¸âƒ£ DDP Dockerfile
# containers/ddp/Dockerfile
FROM nvcr.io/nvidia/pytorch:24.01-py3

LABEL description="PyTorch DDP Training Container"

# DDP ê´€ë ¨ íŒ¨í‚¤ì§€
RUN pip install --no-cache-dir \
    torch==2.2.0 \
    torchvision==0.17.0 \
    transformers==4.37.0 \
    datasets==2.16.0 \
    wandb==0.16.2 \
    tensorboard==2.15.0

# NCCL ìµœì í™” ì„¤ì •
ENV NCCL_DEBUG=INFO
ENV NCCL_IB_DISABLE=0
ENV NCCL_NET_GDR_LEVEL=5

# ì½”ë“œ ë³µì‚¬
COPY training/ddp/ /workspace/ddp/
WORKDIR /workspace/ddp

ENTRYPOINT ["python"]

2ï¸âƒ£ Megatron-LM Dockerfile
# containers/megatron/Dockerfile
FROM nvcr.io/nvidia/pytorch:24.01-py3

LABEL description="Megatron-LM Training Container"

# Apex ì„¤ì¹˜ (í˜¼í•© ì •ë°€ë„ í•™ìŠµ)
RUN git clone https://github.com/NVIDIA/apex && \
    cd apex && \
    pip install -v --disable-pip-version-check --no-cache-dir \
    --no-build-isolation \
    --config-settings "--build-option=--cpp_ext" \
    --config-settings "--build-option=--cuda_ext" ./

# Megatron-LM ì„¤ì¹˜
RUN git clone https://github.com/NVIDIA/Megatron-LM.git /workspace/Megatron-LM && \
    cd /workspace/Megatron-LM && \
    pip install -e .

# ì¶”ê°€ ì˜ì¡´ì„±
RUN pip install --no-cache-dir \
    nltk==3.8.1 \
    sentencepiece==0.1.99 \
    wandb==0.16.2

# NCCL ë° Megatron ìµœì í™”
ENV NCCL_DEBUG=INFO
ENV CUDA_DEVICE_MAX_CONNECTIONS=1
ENV NVTE_ALLOW_NONDETERMINISTIC_ALGO=1

WORKDIR /workspace/Megatron-LM

ENTRYPOINT ["python"]

3ï¸âƒ£ DeepSpeed Dockerfile
# containers/deepspeed/Dockerfile
FROM nvcr.io/nvidia/pytorch:24.01-py3

LABEL description="DeepSpeed Training Container"

# DeepSpeed ì„¤ì¹˜
RUN pip install --no-cache-dir \
    deepspeed==0.13.1 \
    transformers==4.37.0 \
    datasets==2.16.0 \
    accelerate==0.26.0 \
    wandb==0.16.2

# DeepSpeed ìµœì í™” ë¹Œë“œ (ì„ íƒ)
RUN DS_BUILD_OPS=1 pip install deepspeed --no-cache-dir --force-reinstall

# NCCL ì„¤ì •
ENV NCCL_DEBUG=INFO
ENV NCCL_IB_DISABLE=0

# ì½”ë“œ ë³µì‚¬
COPY training/deepspeed/ /workspace/deepspeed/
WORKDIR /workspace/deepspeed

ENTRYPOINT ["deepspeed"]

ğŸš€ 2. ECRì— ë¹Œë“œ & í‘¸ì‹œ
#!/bin/bash
# scripts/build_all_frameworks.sh

set -e

AWS_ACCOUNT_ID="123456789012"
AWS_REGION="us-east-1"
ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"

# ECR ë¡œê·¸ì¸
aws ecr get-login-password --region ${AWS_REGION} | \
    docker login --username AWS --password-stdin ${ECR_REGISTRY}

# í”„ë ˆì„ì›Œí¬ ëª©ë¡
FRAMEWORKS=("ddp" "megatron" "deepspeed")

for FW in "${FRAMEWORKS[@]}"; do
    echo ""
    echo "=== Building ${FW} ==="
    
    # ECR ë¦¬í¬ì§€í† ë¦¬ ìƒì„±
    aws ecr describe-repositories \
        --repository-names "ml-frameworks/${FW}" \
        --region ${AWS_REGION} 2>/dev/null || \
    aws ecr create-repository \
        --repository-name "ml-frameworks/${FW}" \
        --region ${AWS_REGION}
    
    # Docker ë¹Œë“œ
    docker build \
        -t ${FW}:latest \
        -f containers/${FW}/Dockerfile \
        .
    
    # íƒœê·¸ & í‘¸ì‹œ
    docker tag ${FW}:latest \
        ${ECR_REGISTRY}/ml-frameworks/${FW}:latest
    
    docker push ${ECR_REGISTRY}/ml-frameworks/${FW}:latest
    
    echo "âœ“ ${FW} ì´ë¯¸ì§€ í‘¸ì‹œ ì™„ë£Œ"
done

ğŸ“¥ 3. Enrootë¡œ Import
#!/bin/bash
# scripts/import_frameworks.sh

set -e

AWS_ACCOUNT_ID="123456789012"
AWS_REGION="us-east-1"
ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"

# ECR ì¸ì¦
aws ecr get-login-password --region ${AWS_REGION} | \
    enroot-credentials set \
    --username AWS \
    --password-stdin \
    ${ECR_REGISTRY}

mkdir -p containers

# Import
FRAMEWORKS=("ddp" "megatron" "deepspeed")

for FW in "${FRAMEWORKS[@]}"; do
    echo "=== Importing ${FW} ==="
    
    enroot import \
        --output containers/${FW}.sqsh \
        docker://${ECR_REGISTRY}/ml-frameworks/${FW}:latest
    
    echo "âœ“ ${FW}.sqsh ìƒì„± ì™„ë£Œ"
done

ls -lh containers/*.sqsh

ğŸ“ 4. ê° í”„ë ˆì„ì›Œí¬ë³„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
1ï¸âƒ£ PyTorch DDP í•™ìŠµ
#!/bin/bash
# scripts/train_ddp.sh

#SBATCH --job-name=ddp-training
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=12
#SBATCH --exclusive
#SBATCH --output=logs/ddp/%x_%j.out

export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)
export MASTER_PORT=29500

declare -a CONTAINER_ARGS=(
    --container-image=${PWD}/containers/ddp.sqsh
    --container-mounts=${PWD}/data:/data,${PWD}/checkpoints:/checkpoints
)

declare -a TORCHRUN_ARGS=(
    --nproc_per_node=8
    --nnodes=$SLURM_JOB_NUM_NODES
    --node_rank=$SLURM_NODEID
    --rdzv_id=$SLURM_JOB_ID
    --rdzv_backend=c10d
    --rdzv_endpoint=$MASTER_ADDR:$MASTER_PORT
)

# DDP í•™ìŠµ ì‹¤í–‰
srun -l "${CONTAINER_ARGS[@]}" \
    torchrun "${TORCHRUN_ARGS[@]}" \
    train_ddp.py \
    --model-name gpt2-medium \
    --data-path /data/wikitext \
    --checkpoint-dir /checkpoints/ddp \
    --batch-size 32 \
    --epochs 10 \
    --lr 3e-4

2ï¸âƒ£ Megatron-LM í•™ìŠµ
#!/bin/bash
# scripts/train_megatron.sh

#SBATCH --job-name=megatron-training
#SBATCH --nodes=8
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=12
#SBATCH --exclusive
#SBATCH --output=logs/megatron/%x_%j.out

export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)
export MASTER_PORT=29500

# Megatron í™˜ê²½ ë³€ìˆ˜
export CUDA_DEVICE_MAX_CONNECTIONS=1
export NCCL_IB_DISABLE=0

declare -a CONTAINER_ARGS=(
    --container-image=${PWD}/containers/megatron.sqsh
    --container-mounts=${PWD}/data:/data,${PWD}/checkpoints:/checkpoints
)

# Megatron í•™ìŠµ íŒŒë¼ë¯¸í„°
TENSOR_PARALLEL=4      # í…ì„œ ë³‘ë ¬í™” í¬ê¸°
PIPELINE_PARALLEL=2    # íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” í¬ê¸°
DATA_PARALLEL=8        # ë°ì´í„° ë³‘ë ¬í™” í¬ê¸° (ìë™ ê³„ì‚°: 64 GPUs / 4 / 2)

GLOBAL_BATCH_SIZE=2048
MICRO_BATCH_SIZE=4

# Megatron ì‹¤í–‰
srun -l "${CONTAINER_ARGS[@]}" \
    pretrain_gpt.py \
    --tensor-model-parallel-size $TENSOR_PARALLEL \
    --pipeline-model-parallel-size $PIPELINE_PARALLEL \
    --num-layers 24 \
    --hidden-size 2048 \
    --num-attention-heads 16 \
    --seq-length 2048 \
    --max-position-embeddings 2048 \
    --micro-batch-size $MICRO_BATCH_SIZE \
    --global-batch-size $GLOBAL_BATCH_SIZE \
    --train-iters 100000 \
    --lr 1.5e-4 \
    --min-lr 1.0e-5 \
    --lr-decay-style cosine \
    --lr-warmup-iters 2000 \
    --weight-decay 0.1 \
    --clip-grad 1.0 \
    --fp16 \
    --data-path /data/megatron/my-gpt2_text_document \
    --vocab-file /data/megatron/gpt2-vocab.json \
    --merge-file /data/megatron/gpt2-merges.txt \
    --save /checkpoints/megatron \
    --load /checkpoints/megatron \
    --save-interval 1000 \
    --eval-interval 100 \
    --eval-iters 10 \
    --distributed-backend nccl

3ï¸âƒ£ DeepSpeed í•™ìŠµ
#!/bin/bash
# scripts/train_deepspeed.sh

#SBATCH --job-name=deepspeed-training
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=12
#SBATCH --exclusive
#SBATCH --output=logs/deepspeed/%x_%j.out

export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n1)
export MASTER_PORT=29500

declare -a CONTAINER_ARGS=(
    --container-image=${PWD}/containers/deepspeed.sqsh
    --container-mounts=${PWD}/data:/data,${PWD}/checkpoints:/checkpoints,${PWD}/configs:/configs
)

# DeepSpeed hostfile ìƒì„±
cat > /tmp/hostfile_${SLURM_JOB_ID} << EOF
$(scontrol show hostname $SLURM_NODELIST | awk '{print $1" slots=8"}')
EOF

# DeepSpeed ì‹¤í–‰
srun -l "${CONTAINER_ARGS[@]}" \
    --launcher=pytorch \
    train_deepspeed.py \
    --deepspeed \
    --deepspeed_config /configs/ds_config_zero3.json \
    --model-name gpt2-large \
    --data-path /data/wikitext \
    --output-dir /checkpoints/deepspeed \
    --per-device-train-batch-size 8 \
    --gradient-accumulation-steps 4 \
    --num-train-epochs 3 \
    --learning-rate 5e-5 \
    --fp16 \
    --logging-steps 10 \
    --save-steps 1000

âš™ï¸ 5. DeepSpeed ì„¤ì • íŒŒì¼ë“¤
ZeRO Stage 1 (Optimizer State Partitioning)
{
  "train_batch_size": 256,
  "train_micro_batch_size_per_gpu": 8,
  "gradient_accumulation_steps": 4,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 3e-4,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 3e-4,
      "warmup_num_steps": 1000
    }
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "gradient_clipping": 1.0,
  "steps_per_print": 10,
  "wall_clock_breakdown": false
}

ZeRO Stage 2 (+ Gradient Partitioning)
{
  "train_batch_size": 256,
  "train_micro_batch_size_per_gpu": 4,
  "gradient_accumulation_steps": 8,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 3e-4,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "gradient_clipping": 1.0
}

ZeRO Stage 3 (+ Parameter Partitioning)
{
  "train_batch_size": 256,
  "train_micro_batch_size_per_gpu": 2,
  "gradient_accumulation_steps": 16,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 3e-4,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    },
    "overlap_comm": true,
    "contiguous_gradients": true,
    "sub_group_size": 1e9,
    "reduce_bucket_size": 5e8,
    "stage3_prefetch_bucket_size": 5e8,
    "stage3_param_persistence_threshold": 1e6,
    "stage3_max_live_parameters": 1e9,
    "stage3_max_reuse_distance": 1e9,
    "stage3_gather_16bit_weights_on_model_save": true
  },
  "gradient_clipping": 1.0,
  "activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": true,
    "contiguous_memory_optimization": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
  }
}

ğŸ“Š 6. ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ
í”„ë ˆì„ì›Œí¬ ì„ íƒ ê¸°ì¤€
í”„ë ˆì„ì›Œí¬	ëª¨ë¸ í¬ê¸°	GPU ë©”ëª¨ë¦¬	í†µì‹  ì˜¤ë²„í—¤ë“œ	êµ¬í˜„ ë‚œì´ë„
DDP	~1B params	ì¶©ë¶„	ë‚®ìŒ	â­ ì‰¬ì›€
Megatron	10B+ params	ë¶€ì¡±	ì¤‘ê°„	â­â­â­â­ ì–´ë ¤ì›€
DeepSpeed	1B~100B+	ë¶€ì¡±	ë‚®ìŒ	â­â­â­ ì¤‘ê°„
ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
DDP ì‚¬ìš©:
âœ… ëª¨ë¸ì´ ë‹¨ì¼ GPUì— ë“¤ì–´ê°
âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ í•„ìš”
âœ… ê°„ë‹¨í•œ ë°ì´í„° ë³‘ë ¬í™”ë§Œ í•„ìš”
ì˜ˆ: ResNet, BERT-Base, GPT-2 Small

Megatron ì‚¬ìš©:
âœ… ì´ˆëŒ€ê·œëª¨ ëª¨ë¸ (10B+ parameters)
âœ… í…ì„œ/íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” í•„ìš”
âœ… NVIDIA GPU ì¸í”„ë¼
ì˜ˆ: GPT-3, Megatron-Turing NLG

DeepSpeed ì‚¬ìš©:
âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ì¤‘ìš”
âœ… ë‹¤ì–‘í•œ ë³‘ë ¬í™” ê¸°ë²• ì¡°í•©
âœ… CPU ì˜¤í”„ë¡œë”© í™œìš©
ì˜ˆ: GPT-Neo, BLOOM, OPT

ğŸ”§ 7. í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# scripts/train_all_frameworks.sh

set -e

echo "=== Multi-Framework Training Pipeline ==="
echo ""

# 1. ì»¨í…Œì´ë„ˆ ì¤€ë¹„ í™•ì¸
echo "Step 1: Checking containers..."
for fw in ddp megatron deepspeed; do
    if [ ! -f "containers/${fw}.sqsh" ]; then
        echo "Error: ${fw}.sqsh not found"
        exit 1
    fi
done
echo "âœ“ All containers ready"

# 2. ë¡œê·¸ ë””ë ‰í† ë¦¬
mkdir -p logs/{ddp,megatron,deepspeed}

# 3. ì‘ì—… ì œì¶œ
echo ""
echo "Step 2: Submitting jobs..."

# DDP (ì‘ì€ ëª¨ë¸, ë¹ ë¥¸ ì‹¤í–‰)
JOB_DDP=$(sbatch --parsable scripts/train_ddp.sh)
echo "  DDP:       Job $JOB_DDP"

# Megatron (í° ëª¨ë¸, ë§ì€ ë¦¬ì†ŒìŠ¤)
JOB_MEGA=$(sbatch --parsable scripts/train_megatron.sh)
echo "  Megatron:  Job $JOB_MEGA"

# DeepSpeed (ë©”ëª¨ë¦¬ íš¨ìœ¨)
JOB_DS=$(sbatch --parsable scripts/train_deepspeed.sh)
echo "  DeepSpeed: Job $JOB_DS"

# 4. ëª¨ë‹ˆí„°ë§
echo ""
echo "Step 3: Monitoring..."
squeue -j $JOB_DDP,$JOB_MEGA,$JOB_DS

# 5. ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸ (ì„ íƒ)
echo ""
echo "ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸ (Ctrl+Cë¡œ ì¢…ë£Œ):"
echo "  tail -f logs/ddp/*_${JOB_DDP}.out"
echo "  tail -f logs/megatron/*_${JOB_MEGA}.out"
echo "  tail -f logs/deepspeed/*_${JOB_DS}.out"

ğŸ¯ 8. ì„±ëŠ¥ ìµœì í™” íŒ
DDP ìµœì í™”
# train_ddp.py
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 1. Gradient bucketing
model = DDP(
    model,
    device_ids=[local_rank],
    bucket_cap_mb=25,  # ê·¸ë˜ë””ì–¸íŠ¸ ë²„í‚· í¬ê¸°
    find_unused_parameters=False  # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì°¾ê¸° ë¹„í™œì„±í™”
)

# 2. í˜¼í•© ì •ë°€ë„
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

for batch in dataloader:
    with autocast():
        loss = model(batch)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

# 3. Gradient accumulation
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

Megatron ìµœì í™”
# ìµœì ì˜ ë³‘ë ¬í™” ì¡°í•© ì°¾ê¸°
# TP Ã— PP Ã— DP = Total GPUs

# ì˜ˆ: 64 GPUs
--tensor-model-parallel-size 8    # TP=8
--pipeline-model-parallel-size 4  # PP=4
# DP = 64 / (8 Ã— 4) = 2 (ìë™)

# Activation checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
--checkpoint-activations \
--checkpoint-num-layers 1

DeepSpeed ìµœì í™”
{
  "zero_optimization": {
    "stage": 3,
    "overlap_comm": true,           // í†µì‹  ì˜¤ë²„ë©
    "contiguous_gradients": true,   // ê·¸ë˜ë””ì–¸íŠ¸ ì—°ì† ë©”ëª¨ë¦¬
    "sub_group_size": 1e9,          // ì„œë¸Œ ê·¸ë£¹ í¬ê¸°
    "reduce_bucket_size": 5e8,      // Reduce ë²„í‚· í¬ê¸°
    "stage3_prefetch_bucket_size": 5e8,  // Prefetch ë²„í‚·
    "stage3_max_live_parameters": 1e9,   // ìµœëŒ€ ë¼ì´ë¸Œ íŒŒë¼ë¯¸í„°
    "stage3_max_reuse_distance": 1e9     // ì¬ì‚¬ìš© ê±°ë¦¬
  },
  "comms_logger": {
    "enabled": true,
    "verbose": false,
    "prof_all": false,
    "debug": false
  }
}

ğŸ“ˆ 9. ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…
í†µí•© ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
# scripts/monitor_all.sh

watch -n 5 '
echo "=== Running Jobs ==="
squeue -u $USER -o "%.10i %.15j %.8T %.10M %.6D %.20R"
echo ""

echo "=== GPU Utilization ==="
for job in $(squeue -u $USER -h -o "%i"); do
    echo "Job $job:"
    srun --jobid=$job --overlap nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader 2>/dev/null | head -n 4
    echo ""
done

echo "=== Latest Logs ==="
for fw in ddp megatron deepspeed; do
    latest=$(ls -t logs/${fw}/*.out 2>/dev/null | head -n1)
    if [ -f "$latest" ]; then
        echo "${fw^^}: $(tail -n 1 $latest)"
    fi
done
'

ğŸ“ ìµœì¢… ì •ë¦¬
ì „ì²´ ì›Œí¬í”Œë¡œìš°
1. Dockerfile ì‘ì„± (ê° í”„ë ˆì„ì›Œí¬ë³„) â”œâ”€ ddp/Dockerfile â”œâ”€ megatron/Dockerfile â””â”€ deepspeed/Dockerfile

2. ECRì— ë¹Œë“œ & í‘¸ì‹œ â””â”€ bash scripts/build_all_frameworks.sh

3. Enrootë¡œ import â””â”€ bash scripts/import_frameworks.sh

4. í•™ìŠµ ì‹¤í–‰
